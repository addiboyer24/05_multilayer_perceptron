{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that you're a wildlife biologist, and you're conducting a study on the activity habits of whitetail deer.  \n",
    "<img src=\"images/deer.jpg\">\n",
    "As any hunter knows, deer activity varies depending on time of day, and this time-varying level of activity is what you're tasked with quantifying.  In particular, what you want to do is to fit a model that asks the question: if I am sitting at a particular spot at a particular time of day, what is the probability that I will see a whitetail deer?  \n",
    "\n",
    "In principle, this is precisely the type of question that we might be interested in using logistic regression for.  We have a feature $\\mathbf{x}$, which is the time of day, as well as a binary outcome $y\\in\\{0,1\\}$ (whether we see a deer or not) for which we'd like to generate a probability.  What do we need to fit this model?  \n",
    "\n",
    "Of course, to begin with we'll need a dataset.  To collect such a dataset we could simply put out a camera (strapped to a tree, for example), set it to take pictures at random and determine whether there is a deer in the image (we could determine this by hand or using a machine learning algorithm to automatically process the image, something that we'll get to in this course!).  In full disclosure, the dataset that I'll provide here is synthetically generated (i.e. it's from a simulation).  However, let's suspend disbelief and imagine that we've done what we described above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = [18,15]\n",
    "mpl.rcParams['font.size'] = 18\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x,y = np.loadtxt('datasets/deer.npy')\n",
    "x = x[::5]\n",
    "y = y[::5]\n",
    "idx = np.argsort(x)\n",
    "x = x[idx]\n",
    "y = y[idx]\n",
    "y_obs = np.reshape(y,(len(y),1))\n",
    "\n",
    "\n",
    "\n",
    "plt.hist(x[y==0],bins=24,histtype='step',label='no deer')\n",
    "plt.hist(x[y==1],bins=24,histtype='step',label='deer')\n",
    "plt.xlabel('Normalized time')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'd like to fit a logistic regression model.  As with the lobster problem, it's helpful to have normalized features so that gradient descent converges easily.  We can basically lift the code from that problem for running gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(a):\n",
    "    return 1./(1+np.exp(-a))\n",
    "\n",
    "def L(y_obs,y_pred):\n",
    "    return -1./len(y_obs)*np.sum(y_obs*np.log(y_pred) + (1-y_obs)*np.log(1-y_pred))\n",
    "\n",
    "def grad(y_obs,y_pred,phi):\n",
    "    return phi.T @ (y_pred-y_obs)\n",
    "\n",
    "w = np.random.randn(2,1)\n",
    "phi = np.vstack((np.ones_like(x),x)).T\n",
    "\n",
    "eta = 1e-4\n",
    "for i in range(50):\n",
    "    y_pred = sigmoid(phi @ w)\n",
    "    w -= eta*grad(y_obs,y_pred,phi)  \n",
    "    print(L(y_obs,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_obs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty easy problem, so gradient descent converges in just a handful of iterations.  Let's plot our predictions.  We'll just plot the histograms from before, as well as the outut of the sigmoid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(x[y==0],bins=24,histtype='step',label='no deer')\n",
    "plt.hist(x[y==1],bins=24,histtype='step',label='deer')\n",
    "plt.xlabel('Normalized time')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "ax = plt.twinx()\n",
    "ax.plot(x,y_pred,'k.')\n",
    "ax.set_ylabel('Probability of deer sighting')\n",
    "ax.set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh.\n",
    "\n",
    "It appears to be the case that logistic regression is telling us that there always about a 40% chance of seeing a deer, regardless of the time of day.  While this is a bit underwhelming, logistic regression is actually performing as well as can be expected in this case: gradient descent has been successful, and this is indeed the optimized model.  Please answer the following two questions:\n",
    "- **Why is the model predicting a uniform value for the probability, even though the data clearly show probability as a function of time?**\n",
    "- **Why does the model predict 40% everywhere?**\n",
    "\n",
    "We can perhaps gain some insight here by attempting to adjust the sigmoid by hand.  Let's start with some random one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(x[y==1],bins=24,histtype='step',label='deer')\n",
    "plt.xlabel('Normalized time')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "\n",
    "ax = plt.twinx()\n",
    "w_random = np.array([-5,10])\n",
    "y_pred = sigmoid(phi@w_random)\n",
    "ax.plot(x,y_pred,'k.')\n",
    "ax.set_ylabel('Probability of deer sighting')\n",
    "ax.set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want this curve to be high whenever the sighting count is high and low otherwise.  We have two modes of adjustment.  We can vary $w_0$, which does this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(x[y==1],bins=24,histtype='step',label='deer')\n",
    "plt.xlabel('Normalized time')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "\n",
    "ax = plt.twinx()\n",
    "for w_0 in range(-8,-3):\n",
    "    w_random = np.array([w_0,10])\n",
    "    y_pred = sigmoid(phi@w_random)\n",
    "    ax.plot(x,y_pred,'.')\n",
    "ax.set_ylabel('Probability of deer sighting')\n",
    "ax.set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can adjust $w_1$, which does this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(x[y==1],bins=24,histtype='step',label='deer')\n",
    "plt.xlabel('Normalized time')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "\n",
    "ax = plt.twinx()\n",
    "for w_1 in range(5,15,2):\n",
    "    w_random = np.array([-5,w_1])\n",
    "    y_pred = sigmoid(phi@w_random)\n",
    "    ax.plot(x,y_pred,'.')\n",
    "ax.set_ylabel('Probability of deer sighting')\n",
    "ax.set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is there a good way to adjust the logistic function using these two knobs such that it gives a good prediction everywhere?**  The answer, of course, is no: the bimodality of the data makes it so that it's quite impossible for the logistic function, which can only split the domain into one region each of high probability and low probability, to fit well.  As such, the optimization procedure essentially throws up its hands and says \"the best I can do is to default to the prior distribution, which is about a 40% chance of success.  This is pretty much what will *always* happen with logistic regression whenever there isn't a single point that represents a sensible decision boundary between the success and failure class (this boundary becomes a line in 2D, a plane in 3D, and so on).\n",
    "\n",
    "## 5.2 Data transformation\n",
    "The problem isn't necessarily hopeless though.  Notice that the features are fixed and known.  As such, we could try to transform the features into something that is more amenable to the assumptions of logistic regression.  To put this concretely, rather than using $x$ (the time) as a feature, we could use $f(x)$ as a feature, where $f(x)$ is some function that we can choose.  For example, we could hypothesize that maybe taking the negative exponential of the feature might yield a better behaved problem: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat = np.exp(-x)\n",
    "plt.hist(xhat[y==0],bins=24,histtype='step',label='no deer')\n",
    "plt.hist(xhat[y==1],bins=24,histtype='step',label='deer')\n",
    "plt.xlabel('f(Normalized time)')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But of course it doesn't, because $f(x) = \\mathrm{e}^{-x}$ is monotonic.  **Come up with some function that would transform the data such that the notion of a single, pointwise decision boundary becomes more reasonable.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\" Define a function that transforms the data into something that can be used with logistic regression.\"\"\"\n",
    "    return np.ones_like(x)  # This is just a placeholder\n",
    "\n",
    "xhat = f(x)\n",
    "plt.hist(xhat[y==0],bins=24,histtype='step',label='no deer')\n",
    "plt.hist(xhat[y==1],bins=24,histtype='step',label='deer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/fighting.jpg\">\n",
    "\n",
    "Once you've completed the above exercise, you can look at the very reasonable choice that I've suggested below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat = np.cos(4*np.pi*x)\n",
    "plt.hist(xhat[y==0],bins=24,histtype='step',label='no deer')\n",
    "plt.hist(xhat[y==1],bins=24,histtype='step',label='deer')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By taking the cosine (adjusted for twice-daily surge of deer activity), we transform the data into two unimodal populations, which is just what we'd like for using logistic regression.  In fact, we can fit this dataset using logistic regression pretty well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.randn(2,1)\n",
    "phi = np.vstack((np.ones_like(xhat),xhat)).T\n",
    "\n",
    "eta = 1e-3\n",
    "for i in range(50):\n",
    "    y_pred = sigmoid(phi @ w)\n",
    "    w -= eta*grad(y_obs,y_pred,phi)  \n",
    "    print(L(y_obs,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(xhat[y==0],bins=24,histtype='step',label='no deer')\n",
    "plt.hist(xhat[y==1],bins=24,histtype='step',label='deer')\n",
    "plt.xlabel('cos(4 pi x)')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "ax = plt.twinx()\n",
    "ax.plot(xhat,y_pred,'k.')\n",
    "ax.set_ylabel('Probability of deer sighting')\n",
    "ax.set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In $f(x)=\\mathrm{cos \\;4\\pi x}$-space, we get a pretty nice fit to the data.  However, it's easier to interpret back in normal $x$ space.  Since the transformation between $x$ and $f(x)$ is fixed and known, it's easy to plot the probabilities as a function of $x$, rather than as a function of $f(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(x[y==0],bins=24,histtype='step',label='no deer')\n",
    "plt.hist(x[y==1],bins=24,histtype='step',label='deer')\n",
    "plt.xlabel('Normalized time')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "ax = plt.twinx()\n",
    "ax.plot(x,y_pred,'k.')\n",
    "ax.set_ylabel('Probability of deer sighting')\n",
    "ax.set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty nice.  We've established that even though logistic regression doesn't work very well for certain patterns of data (i.e. bimodal data), it can work very well if we can transform the features such that the assumptions of logistic regression are satisfied.\n",
    "\n",
    "## 5.3 The trouble\n",
    "While data transformation is very powerful, using it as we did above becomes problematic in more challenging cases.  In particular, our capacity to use expert judgement to come up with a sensible function to use as a transformation mechanism becomes prohibitive as the dimensionality of the data becomes large.  Thus, we need to come up with an *automatic* method for transforming this data such that it is amenable for use in logistic (or softmax) regression.\n",
    "\n",
    "## 5.4 Graphical models\n",
    "Before we tackle the problem of automatic data transformation, it will be useful to write our models graphically, just to keep things straight.  We'll write square nodes for input features, and lines indicating multiplication, with the coefficient of that multiplication written above the line.  Circular nodes will represent a function that takes as input $a$, which is a sum of all the inputs going to that node, and outputs $z$.  With this notation we can easily write normal logistic regression as\n",
    "<img src=\"addme.jpg\">\n",
    "The model that we came up with above would be written as\n",
    "<img src=\"addme.jpg\">\n",
    "If we're interested in outputting an objective function value from our graph instead of a prediction, that just adds another node onto the end of the graph.  \n",
    "\n",
    "## 5.5 Hidden layers\n",
    "Each of these models are in fact (very simple) neural networks, where we map directly from input to prediction to cost function, by the way of these simple activation functions: the identity corresponds to values on the real line, the sigmoid to predicting binary classses, etc.  These activations act on fixed basis functions specified by the input: the ones column, the $x_1$ column, etc.  What if we decided to incorporate an intermediate layer that could somehow mix up the basis functions into alternative basis functions that we would then feed to our activation.  For example, what if instead of just feeding weighted combinations of features to the logistic function, we instead preprocessed those data into a different set of basis functions?  Perhaps we wouldn't be able to understand exactly what those basis functions were, but they would give our model more flexibility.  \n",
    "\n",
    "Let's introduce another set of activation functions that our inputs pass through before they reach the sigmoid.  This layer of nodes is referred to as a \\emph{hidden layer}.  In principle, we could have as many as we want.  These activation functions \\emph{must} be non-linear for this to be useful.  **What happens when we use the identity as an activation function in this intermediate layer?:**  \n",
    "\n",
    "In practice, using the logistic function itself is a popular choice, but there are many(!) other possibilities.  The primary use of this is that a linear combination of sigmoids is a *universal function approximator*, which is to say that if we take the linear combination of enough of these things, then we can approximate any function that we want to.  For example, what if we wanted to approximate a bell curve: we could use two sigmoids with opposite signs and different translations, which would add up to something approximating a bell curve.  What happens if we wanted to approximate a line?  This wouldn't work because there are these plateaus, right?  It still works, because we could just make one of our sigmoids highly diffuse, so that only this roughly linear interior section is in play, then simply ignore the second one by setting its output weight to zero.  Two sigmoids actually have the capacity to roughly mimic pretty much anything that is linear of unimodal:  pretty much anything with one peak or less.  If we increase the number of nodes in the hidden layer, then we can mimic even more functions, like a sinusoid: for our deer problem, we can use a bank of sigmoids to approximate the sinusoidal transformation we came up with heuristically above!  In particular, we'll use the following graph\n",
    "<img src=\"addme.jpg\">\n",
    "\n",
    "Note that this structure is often called a multi-layer perceptron.  This is sort of an unfortunate name, because it does not, in fact use perceptrons anywhere.  A perceptron is just a funny name for Heaviside regression, which is just like logistic regression, but instead of using the regression function, it uses the heaviside function.  It turned out to not be a particularly useful method for classification, but for some reason the name stuck when it came to neural networks.    \n",
    "\n",
    "## 5.6 Feedforward neural network\n",
    "The complexity of this graph belies the fact that it's actually pretty simple to write a mathematical expression here.  The computation proceeds sequentially\n",
    "$$\n",
    "A^{(1)} = X W^{(1)} + b^{(1)}\n",
    "$$\n",
    "$$\n",
    "Z^{(1)} = \\sigma(A^{(1)})\n",
    "$$\n",
    "$$\n",
    "A^{(2)} = Z^{(1)} W^{(2)} + b^{(2)}\n",
    "$$\n",
    "$$\n",
    "Z^{(2)} = y_{pred} = \\sigma(A^{(2)}).\n",
    "$$\n",
    "**work out the dimensions of this computation, and ensure that they make sense**.  Running a set of features $X$ through this set of linear and non-linear functional compositions leads to a binary prediction, just as in logistic regression.  But how do we train this model?  We now have two matrices of parameters $W$ (of different sizes).  How do we proceed?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(x)\n",
    "n_0 = 1\n",
    "n_1 = 4\n",
    "N = 1\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = x.reshape((m,n_0))\n",
    "\n",
    "W1 = 10*np.random.randn(n_0,n_1)\n",
    "W2 = 10*np.random.randn(n_1,N)\n",
    "\n",
    "b1 = 10*np.random.randn(1,n_1)#np.array([0.,4,8,12.])\n",
    "b2 = 10*np.random.randn(1,N)\n",
    "\n",
    "\n",
    "def sigmoid(a):\n",
    "    return 1./(1+np.exp(-a))\n",
    "\n",
    "def L(y_obs,y_pred):\n",
    "    return -np.sum(y_obs*np.log(y_pred) + (1-y_obs)*np.log(1-y_pred))\n",
    "\n",
    "def grad(y_obs,y_pred,phi):\n",
    "    return phi.T @ (y_pred-y_obs)\n",
    "\n",
    "\n",
    "def feedforward(X,W1,W2,b1,b2):\n",
    "    # Feedforward\n",
    "    A1 = X@W1 + b1\n",
    "    Z1 = sigmoid(A1)\n",
    "    A2 = Z1@W2 + b2\n",
    "    y_pred = sigmoid(A2)\n",
    "    return y_pred,Z1\n",
    "\n",
    "y_pred,Z1 = feedforward(X,W1,W2,b1,b2)\n",
    "print(y_pred)\n",
    "plt.plot(X,y_pred,'k.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Backpropagation\n",
    "\n",
    "Despite the mystique, the neural network is just a model like any other (albeit an extraordinarily flexible one), written as\n",
    "\\begin{equation}\n",
    "y_{pred} = F(X,W).\n",
    "\\end{equation}\n",
    "For it to be useful, we still need to find parameter values: we need to train the model.  Consider the case of logistic regression, but with a hidden layer added.  This thing is really not so different from just vanilla logistic regression: we've added an additional layer of complexity, but the output remains the same, and as such, it's reasonable to assume the same misfit, and also to assume that we can solve this problem via the same technique: gradient descent. \n",
    "\n",
    "We seek the derivative of a cost function with respect to an arbitrary weight in the network\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J}{\\partial W^{(l)}_{ij}}.\n",
    "\\end{equation}\n",
    "As we saw, on graphs like the neural network, we can use the chain rule to propagate changes in misfit back through the network's function transformations, hence the name \\emph{backpropagation}.  For the final layer, we proceed very similarly to how we did for the single layer networks:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J}{\\partial w^{(L)}_{ij}} = \\frac{\\partial J}{\\partial z^{(L)}_{j}} \\frac{\\partial z^{(L)}_{j}}{\\partial a_j^{(L)}} \\frac{\\partial a_j^{(L)}}{\\partial w^{(L)}_{ij}}.\n",
    "\\end{equation}\n",
    "Using our result from above, we have that\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J}{\\partial w^{(L)}_{ij}} = \\underbrace{(z^{(L)}_j - y)}_{\\delta_j^{(L)}} z_{i}^{(L-1)},\n",
    "\\end{equation}\n",
    "where we've specified the value $\\delta_j^{(l)}$, which is the magnitude of the error signal propagated to $w_{ij}$.  With that in mind, we have \n",
    "\\begin{equation}\n",
    "\\frac{\\partial J}{\\partial w^{(L)}_{ij}} = \\underbrace{\\delta_j^{(L)}}_{\\text{backprop. error}} \\underbrace{z_{i}^{(L-1)}}_{\\text{forward prop. magnitude}}.\n",
    "\\end{equation}\n",
    "\n",
    "For deeper network layers, the situation is marginally more complex.  For a weight $w_{ij}^{(l)}$, $l\\neq L$, we perform the same chain rule differentiation:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J}{\\partial w^{(l)}_{ij}} = \\frac{\\partial J}{\\partial z^{(l)}_{j}} \\frac{\\partial z^{(l)}_{j}}{\\partial a_j^{l)}} \\frac{\\partial a_j^{(l)}}{\\partial w^{(l)}_{ij}}.\n",
    "\\end{equation}\n",
    "However, we can't directly compute $\\frac{\\partial J}{\\partial z^{(l)}_{j}}$ anymore, because there are now other nodes in the way.  This isn't a problem however.  We can use the chain rule again, this time to expand  $\\frac{\\partial J}{\\partial z^{(l)}_{j}}$:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J}{\\partial z^{(l)}_{j}} = \\sum_{k=1}^{N^{(l+1)}} \\frac{\\partial J}{\\partial z_k^{(l+1)}}\\frac{\\partial z_k^{(l+1)}}{\\partial a_k^{(l+1)}} \\frac{\\partial a_k^{(l+1)}}{\\partial z_j^{(l)}}.\n",
    "\\end{equation}  \n",
    "This seems quite messy until you recognize that we \\emph{already computed} the first two terms.  We even gave them a name: $\\delta$.  This now simplifies to \n",
    "\\begin{equation}\n",
    "\\frac{\\partial J}{\\partial z^{(l)}_{j}} = \\sum_{k=1}^{N^{(l+1)}} \\delta_k^{(l+1)} w^{(l+1)}_{jk},\n",
    "\\end{equation}\n",
    "where we've also substituted $\\frac{\\partial a_k^{(l+1)}}{\\partial z_j^{(l)}} = w^{(l+1)}_{jk}$.  We can now define a $\\delta$ for the current layer\n",
    "\\begin{equation}\n",
    "\\delta_j^{(l)} = (\\sum_{k=1}^{N^{(l+1)}} \\delta_k^{(l+1)} w^{(l+1)}_{jk}) f_j'(a_j^{(l)}),\n",
    "\\end{equation}\n",
    "where $f_j^{(l)}(\\cdot)$ is the activation function for layer $l$, node $j$, and $f'(a_j^{(l)})$ is its derivative (with respect to $a_j^{(l)}$), which leads to\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J}{\\partial w^{(l)}_{ij}} = \\underbrace{\\delta_j^{(l)}}_{\\text{backprop. error}} \\underbrace{z_{i}^{(l-1)}}_{\\text{forward prop. magnitude}},\n",
    "\\end{equation}\n",
    "which is the same as for layer $L$, just with a different definition for $\\delta$.  It's also even more clear why this is called backpropagation: the gradient for a given layer depends upon the layer in front of it, so the error feeds backwards through the neural network, just as the input feeds forward through the neural network.  There exists this kind of beautiful duality to this structure, where the forward model sweeps from left to right, and the backwards model (sometimes called the adjoint) sweeps from right to left. \n",
    "\n",
    "For implementation on a computer, all those sums are unwieldy and don't allow us to use the fast matrix multiplication libraries that are available.  The tensor version of these equations for softmax with cross-entropy objective function is as follows.  Beginning with the feed forward stage:\n",
    "\\begin{equation}\n",
    "A^{(l)} = Z^{(l-1)} W^{(l)} + B^{(l)}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "Z^{(l)} = \\sigma(A^{(l)})\n",
    "\\end{equation}\n",
    "Note that $Z^{(0)}=X$, where $X$ is the $m \\times n$ feature matrix.  \n",
    "\n",
    "For backpropagation, the tensor form is \n",
    "\\begin{equation}\n",
    "\\nabla_{W^{(l)}} \\mathcal{J} = (Z^{(l-1)})^T\\delta^{(l)}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\nabla_{B^{(l)}} \\mathcal{J} = \\mathbf{1}^T \\delta^{(l)}\n",
    "\\end{equation}\n",
    "where $\\mathbf{1}$ is the $m \\times 1$ vector of ones, and\n",
    "\\begin{equation}\n",
    "\\delta^{(l)} = \\begin{cases} (Z^{(l)} - \\mathcal{T}),\\; \\text{if }l=L \\\\\n",
    "                              \\delta^{(l+1)} (W^{(l+1)})^T \\circ \\sigma'^{(l)}(A^{(l)}),\\;\\text{else}, \\end{cases} \n",
    "\\end{equation}\n",
    "where $\\mathcal{T}$ is the one-hot representation of the data labels, and $\\circ$ represents the Hadamard product (aka elementwise multiplication).  Interestingly, this is also valid for the case of sum square error coupled with an identity activation on the final node, i.e. for regression problems.  In some sense, the gradient of the cost function becomes simple when natural choices of output activation and cost function are chosen.\n",
    "\n",
    "The following function implements the backpropagation formula given above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropogate(y_pred,Z1,X,y_obs):\n",
    "    # Backpropogate\n",
    "    delta_2 = y_pred - y_obs\n",
    "    grad_W2 = Z1.T @ delta_2\n",
    "    grad_b2 = delta_2.sum(axis=0)\n",
    "\n",
    "    delta_1 = delta_2 @ W2.T * Z1*(1-Z1)\n",
    "    grad_W1 = X.T @ delta_1\n",
    "    grad_b1 = delta_1.sum(axis=0) \n",
    "    return grad_W1,grad_W2,grad_b1,grad_b2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have gradients for *several* items: both weight matrices, as well as the bias vectors.  We can just update these separately, but all at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1e-3\n",
    "for i in range(100000):\n",
    "    if i>20000:\n",
    "        eta = 1e-4\n",
    "    y_pred,Z1 = feedforward(X,W1,W2,b1,b2)\n",
    "    grad_W1,grad_W2,grad_b1,grad_b2 = backpropogate(y_pred,Z1,X,y_obs)\n",
    "\n",
    "    W1 -= eta*np.sign(grad_W1)\n",
    "    W2 -= eta*np.sign(grad_W2)\n",
    "    b1 -= eta*np.sign(grad_b1)\n",
    "    b2 -= eta*np.sign(grad_b2)\n",
    "    if i%100==0:\n",
    "        print(i,L(y_obs,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting, how do we do?  We can compare the output of our neural network against the success probability for an empirical histogram of training data (note that this is actually a form of nearest neighbor classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1,b = np.histogram(x[y==1],bins=np.linspace(0,1,48))\n",
    "c2,b = np.histogram(x[y==0],bins=np.linspace(0,1,48))\n",
    "P1 = c1/(c1+c2)\n",
    "print(P1)\n",
    "plt.plot(0.5*(b[1:]+b[:-1]),P1)\n",
    "plt.plot(x,y_pred,'k.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good results for a problem that would have thwarted one of our earlier classifiers.  Let's examine what this thing is doing a little bit more deeply.  It's particularly interesting to look at the outputs of the hidden layer, or what basis functions the model decided to transform the data to before classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    plt.plot(x[::100],Z1[::100,i],'o-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These basis functions represent a transform of our data to a new four dimensional space.  It's instructive to see what we get when we add them up and scale them by some weights that we found with gradient descent: the linear combination of these learned features that get passed to logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Z1@W2 + b2,np.zeros((m)),c=y_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neural network has effectively transformed our dataset into one that can reasonably be classified by the logistic function, just like we did manually via the cosine transform above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
